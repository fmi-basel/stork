{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a deep, recurrent convolutional SNN on the SHD dataset\n",
    "\n",
    "In this notebook, we demonstrate the training of a 3-layer convolutional SNN with recurrent connections in each hidden layer on the [SHD dataset](https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/).\n",
    "\n",
    "We will introduce the use of the `layer` module to initialize feed-forward and recurrent connections at the same time, from the same target parameter $\\sigma_U$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from spiffyplots import MultiPanel\n",
    "\n",
    "import stork\n",
    "\n",
    "import stork.datasets\n",
    "from stork.datasets import HDF5Dataset, DatasetView\n",
    "\n",
    "from stork.models import RecurrentSpikingModel\n",
    "from stork.nodes import InputGroup, ReadoutGroup, LIFGroup, DeltaSynapseLIFGroup\n",
    "from stork.connections import Connection, SuperConvConnection\n",
    "from stork.generators import StandardGenerator\n",
    "from stork.initializers import FluctuationDrivenCenteredNormalInitializer, DistInitializer, KaimingNormalInitializer\n",
    "from stork.layers import ConvLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "***To locally run this notebook on your system, download the SHD dataset from: [https://zenkelab.org/datasets/](https://zenkelab.org/datasets/).***\n",
    "*We need 'shd_train.h5' and 'shd_test.h5'. Move the downloaded files into a folder `data/datasets/hdspikes` in this repo, or change the `datadir` variable below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/tungstenfs/scratch/gzenke/datasets/hdspikes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_inputs = 700\n",
    "orig_duration = 0.7\n",
    "duration = 1\n",
    "time_step = dt = 2e-3\n",
    "nb_time_steps = int(duration / time_step)\n",
    "time_scale = orig_duration / duration\n",
    "unit_scale = 1\n",
    "validation_split = 0.9\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    nb_steps=nb_time_steps,\n",
    "    time_scale=time_scale / time_step,\n",
    "    unit_scale=unit_scale,\n",
    "    nb_units=nb_inputs,\n",
    "    preload=True,\n",
    "    precompute_dense=False,\n",
    "    unit_permutation=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and split dataset into train / validation / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HDF5Dataset(os.path.join(datadir, \"shd_train.h5\"), **gen_kwargs)\n",
    "\n",
    "# Split into train and validation set\n",
    "mother_dataset = train_dataset\n",
    "elements = np.arange(len(mother_dataset))\n",
    "np.random.shuffle(elements)\n",
    "split = int(validation_split * len(mother_dataset))\n",
    "valid_dataset = DatasetView(mother_dataset, elements[split:])\n",
    "train_dataset = DatasetView(mother_dataset, elements[:split])\n",
    "\n",
    "test_dataset = HDF5Dataset(os.path.join(datadir, \"shd_test.h5\"), **gen_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "# # # # # # # # # # #\n",
    "\n",
    "beta = 20\n",
    "nb_hidden_layers = 1\n",
    "nb_classes = 20\n",
    "nb_filters = [16]      # Number of features per layer\n",
    "\n",
    "kernel_size = [21]        # Convolutional operation parameters\n",
    "stride = [10]\n",
    "padding = [0]\n",
    "\n",
    "recurrent_kwargs = {'kernel_size': 5,\n",
    "                    'stride': 1,\n",
    "                    'padding': 2}\n",
    "\n",
    "# Neuron Parameters\n",
    "# # # # # # # # # # #\n",
    "\n",
    "neuron_group = DeltaSynapseLIFGroup\n",
    "tau_mem = 20e-3\n",
    "tau_syn = 10e-3\n",
    "tau_readout = duration\n",
    "\n",
    "# FILTER PARAMETERS\n",
    "# # # # # # # # # # #\n",
    "nb_synaptic_filters = 10\n",
    "tau_filter = 20e-3\n",
    "\n",
    "# Training parameters\n",
    "# # # # # # # # # # #\n",
    "\n",
    "batch_size = 200\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float\n",
    "lr = 5e-3\n",
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SuperSpike and loss function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "act_fn = stork.activations.SuperSpike\n",
    "act_fn.beta = beta\n",
    "\n",
    "loss_stack = stork.loss_stacks.MaxOverTimeCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = stork.optimizers.SMORMS3\n",
    "generator = StandardGenerator(nb_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regularizer parameters (set regularizer strenght to 0 if you don't want to use them)\n",
    "upperBoundL2Strength = 0.01\n",
    "upperBoundL2Threshold = 7       # Regularizes spikecount: 7 spikes ~ 10 Hz in 700ms simulation time\n",
    "\n",
    "# Define regularizer list\n",
    "regs = []\n",
    "\n",
    "regUB = stork.regularizers.UpperBoundL2(upperBoundL2Strength,\n",
    "                                        threshold=upperBoundL2Threshold, \n",
    "                                        dims=-1)\n",
    "regs.append(regUB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializer setup\n",
    "We initialize in the fluctuation-driven regime with a target membrane potential standard deviation $\\sigma_U=1.0$. Additionally, we set the proportion of membrane potential fluctuations driven by feed-forward inputs to $\\alpha=0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_u = 1.0\n",
    "nu = 15.8\n",
    "\n",
    "initializer = FluctuationDrivenCenteredNormalInitializer(\n",
    "    sigma_u = sigma_u, \n",
    "    nu=nu, \n",
    "    timestep = dt,\n",
    "    alpha=0.9\n",
    "    )\n",
    "\n",
    "readout_initializer = DistInitializer(\n",
    "    dist = torch.distributions.Normal(0, 1),\n",
    "    scaling='1/sqrt(k)'\n",
    ")\n",
    "\n",
    "super_initializer = KaimingNormalInitializer(gain=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentSpikingModel(\n",
    "    batch_size,\n",
    "    nb_time_steps,\n",
    "    nb_inputs,\n",
    "    device,\n",
    "    dtype)\n",
    "\n",
    "# INPUT LAYER\n",
    "# # # # # # # # # # # # # # #\n",
    "input_shape = (1, nb_inputs)\n",
    "input_group = model.add_group(InputGroup(input_shape))\n",
    "\n",
    "# Set input group as upstream of first hidden layer\n",
    "upstream_group = input_group\n",
    "\n",
    "# HIDDEN LAYERS\n",
    "# # # # # # # # # # # # # # #\n",
    "neuron_kwargs = {'tau_mem': 20e-3,\n",
    "                 'activation': act_fn}\n",
    "\n",
    "for layer_idx in range(nb_hidden_layers):\n",
    "    \n",
    "    # Generate Layer name and config\n",
    "    layer_name = str('ConvLayer') + ' ' + str(layer_idx)\n",
    "\n",
    "    # Make layer\n",
    "    layer = ConvLayer(name = layer_name, \n",
    "                      model = model,\n",
    "                      input_group = upstream_group,\n",
    "                      kernel_size = kernel_size[layer_idx],\n",
    "                      stride = stride[layer_idx],\n",
    "                      padding = padding[layer_idx],\n",
    "                      nb_filters= nb_filters[layer_idx],\n",
    "                      recurrent = False,\n",
    "                      neuron_class = neuron_group,\n",
    "                      neuron_kwargs = neuron_kwargs,\n",
    "                      connection_class=SuperConvConnection,\n",
    "                      connection_kwargs={'tau_filter': tau_filter,\n",
    "                                         'nb_filters': nb_synaptic_filters,},\n",
    "                      recurrent_connection_kwargs = recurrent_kwargs,\n",
    "                      regs = regs,\n",
    "                      )\n",
    "    \n",
    "    # Initialize Parameters\n",
    "    super_initializer.initialize(layer)\n",
    "    \n",
    "    # Set output as input to next layer\n",
    "    upstream_group = layer.output_group\n",
    "\n",
    "# READOUT LAYER\n",
    "# # # # # # # # # # # # # # #\n",
    "readout_group = model.add_group(ReadoutGroup(\n",
    "    nb_classes,\n",
    "    tau_mem=tau_readout,\n",
    "    tau_syn=tau_syn,\n",
    "    initial_state=-1e-3))\n",
    "\n",
    "readout_connection = model.add_connection(Connection(upstream_group, \n",
    "                                                     readout_group,\n",
    "                                                     flatten_input=True))\n",
    "\n",
    "# Initialize readout connection\n",
    "readout_initializer.initialize(readout_connection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add monitors for spikes and membrane potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_hidden_layers):\n",
    "    model.add_monitor(stork.monitors.SpikeCountMonitor(model.groups[1 + i]))\n",
    "\n",
    "for i in range(nb_hidden_layers):\n",
    "    model.add_monitor(stork.monitors.StateMonitor(model.groups[1 + i], \"out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.configure(input=input_group,\n",
    "                output=readout_group,\n",
    "                loss_stack=loss_stack,\n",
    "                generator=generator,\n",
    "                optimizer=opt,\n",
    "                optimizer_kwargs=dict(lr=lr),\n",
    "                time_step=dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring activity before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150, figsize=(10, 5))\n",
    "stork.plotting.plot_activity_snapshot(\n",
    "    model,\n",
    "    data=test_dataset,\n",
    "    nb_samples=5,\n",
    "    point_alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_weighted_filters = model.connections[0].get_filterbanks(T=duration)\n",
    "\n",
    "# shape = (time, n_post, n_pre, nb_filters, kernel_size)\n",
    "print(before_weighted_filters.shape)\n",
    "\n",
    "# Sum over filter axis\n",
    "before_weighted_filters_sum = before_weighted_filters.sum(axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "takes around 85 minutes using a powerful GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "history = model.fit_validate(\n",
    "    train_dataset,\n",
    "    valid_dataset,\n",
    "    nb_epochs=nb_epochs,\n",
    "    verbose=True)\n",
    "\n",
    "results[\"train_loss\"] = history[\"loss\"].tolist()\n",
    "results[\"train_acc\"] = history[\"acc\"].tolist()\n",
    "results[\"valid_loss\"] = history[\"val_loss\"].tolist()\n",
    "results[\"valid_acc\"] = history[\"val_acc\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_dataset).tolist()\n",
    "results[\"test_loss\"], _, results[\"test_acc\"] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(5,3), dpi=150)\n",
    "\n",
    "for i, n in enumerate([\"train_loss\", \"train_acc\", \"valid_loss\", \"valid_acc\"]):\n",
    "    \n",
    "    if i < 2:\n",
    "        a = ax[0][i]\n",
    "    else:\n",
    "        a = ax[1][i-2]\n",
    "\n",
    "    a.plot(results[n], color=\"black\")\n",
    "    a.set_xlabel(\"Epochs\")\n",
    "    a.set_ylabel(n)\n",
    "\n",
    "ax[0, 1].set_ylim(0, 1)\n",
    "ax[1, 1].set_ylim(0, 1)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Test loss: \", results[\"test_loss\"])\n",
    "print(\"Test acc.: \", results[\"test_acc\"])\n",
    "\n",
    "print(\"\\nValidation loss: \", results[\"valid_loss\"][-1])\n",
    "print(\"Validation acc.: \", results[\"valid_acc\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snapshot after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150, figsize=(10, 5))\n",
    "stork.plotting.plot_activity_snapshot(\n",
    "    model,\n",
    "    data=test_dataset,\n",
    "    nb_samples=5,\n",
    "    point_alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_weighted_filters = model.connections[0].get_filterbanks(T=duration)\n",
    "\n",
    "# shape = (time, n_post, n_pre, nb_filters, kernel_size)\n",
    "print(after_weighted_filters.shape)\n",
    "\n",
    "# Sum over filter axis\n",
    "after_weighted_filters_sum = after_weighted_filters.sum(axis=3)\n",
    "\n",
    "print(after_weighted_filters_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = MultiPanel(grid=[1] * 10, figsize=(2.5, 8), dpi=150)\n",
    "\n",
    "for idx in range(10):\n",
    "    fig.panels[idx].hlines(0, after_weighted_filters_sum.shape[0], 0, color=\"black\", linestyle=\"--\")\n",
    "    fig.panels[idx].plot(after_weighted_filters_sum[:, idx, 0, 0], color='navy')\n",
    "    fig.panels[idx].plot(before_weighted_filters_sum[:, idx, 0, 0], color='crimson')\n",
    "\n",
    "    fig.panels[idx].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4801fe533c28fcaf57028b73c965aa7a759ac2803d659f55fa41a0b68c770499"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('stork-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
